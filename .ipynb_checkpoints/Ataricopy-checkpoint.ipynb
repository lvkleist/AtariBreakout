{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "frames (InputLayer)             (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "normalization (Lambda)          (None, 84, 84, 4)    0           frames[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 20, 20, 16)   4112        normalization[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 9, 9, 32)     8224        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2592)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          663808      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            771         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 3)            0           dense_2[0][0]                    \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 676,915\n",
      "Trainable params: 676,915\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: observe, episode: 0, score: 1.0, global_step: 150, avg loss: 0.0, step: 150, memory length: 150\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = '/home/ubuntu/projects/teamAtari/standard-workflow/notebooks/leo/breakout_model_ep_sec_version.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c9c36b5e9ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c9c36b5e9ab9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m      \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0;31m#test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c9c36b5e9ab9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    287\u001b[0m                     \u001b[0;31m#file_name = \"breakout_model_{}.h5\".format(now)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                     \u001b[0;31m#model_path = os.path.join(FLAGS.train_dir, file_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0;31m# Add user custom data to TensorBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/home/ubuntu/projects/teamAtari/standard-workflow/notebooks/leo/breakout_model_ep_sec_version.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from keras.models import Model\n",
    "\n",
    "from collections import deque\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras.models import clone_model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', 'tf_train_breakout',\n",
    "                           \"\"\"Directory where to write event logs and checkpoint. \"\"\")\n",
    "tf.app.flags.DEFINE_string('restore_file_path',\n",
    "                           '/home/paperspace/fastai/courses/tim/breakout_model_ep_sec_version.h5',\n",
    "                           \"\"\"Path of the restore file \"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_episode', 100000,\n",
    "                            \"\"\"number of epochs of the optimization loop.\"\"\")\n",
    "# tf.app.flags.DEFINE_integer('observe_step_num', 5000,\n",
    "tf.app.flags.DEFINE_integer('observe_step_num', 50000,\n",
    "                            \"\"\"Timesteps to observe before training.\"\"\")\n",
    "# tf.app.flags.DEFINE_integer('epsilon_step_num', 50000,\n",
    "tf.app.flags.DEFINE_integer('epsilon_step_num', 1000000,\n",
    "                            \"\"\"frames over which to anneal epsilon.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('refresh_target_model_num', 10000,  # update the target Q model every refresh_target_model_num\n",
    "                            \"\"\"frames over which to anneal epsilon.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('replay_memory', 400000,  # takes up to 20 GB to store this amount of history data\n",
    "                            \"\"\"number of previous transitions to remember.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('no_op_steps', 30,\n",
    "                            \"\"\"Number of the steps that runs before script begin.\"\"\")\n",
    "tf.app.flags.DEFINE_float('regularizer_scale', 0.01,\n",
    "                          \"\"\"L1 regularizer scale.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('batch_size', 32,\n",
    "                            \"\"\"Size of minibatch to train.\"\"\")\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.00025,\n",
    "                          \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_float('init_epsilon', 1.0,\n",
    "                          \"\"\"starting value of epsilon.\"\"\")\n",
    "tf.app.flags.DEFINE_float('final_epsilon', 0.1,\n",
    "                          \"\"\"final value of epsilon.\"\"\")\n",
    "tf.app.flags.DEFINE_float('gamma', 0.99,\n",
    "                          \"\"\"decay rate of past observations.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('resume', False,\n",
    "                            \"\"\"Whether to resume from previous checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('render', False,\n",
    "                            \"\"\"Whether to display the game.\"\"\")\n",
    "\n",
    "ATARI_SHAPE = (84, 84, 4)  # input image size to model\n",
    "ACTION_SIZE = 3\n",
    "\n",
    "\n",
    "# 210*160*3(color) --> 84*84(mono)\n",
    "# float --> integer (to reduce the size of replay memory)\n",
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe\n",
    "\n",
    "\n",
    "def huber_loss(y, q_value):\n",
    "    error = K.abs(y - q_value)\n",
    "    quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "    linear_part = error - quadratic_part\n",
    "    loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def atari_model():\n",
    "    # With the functional API we need to define the inputs.\n",
    "    frames_input = layers.Input(ATARI_SHAPE, name='frames')\n",
    "    actions_input = layers.Input((ACTION_SIZE,), name='action_mask')\n",
    "\n",
    "    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "    normalized = layers.Lambda(lambda x: x / 255.0, name='normalization')(frames_input)\n",
    "\n",
    "    # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "    conv_1 = layers.convolutional.Conv2D(\n",
    "        16, (8, 8), strides=(4, 4), activation='relu'\n",
    "    )(normalized)\n",
    "    # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "    conv_2 = layers.convolutional.Conv2D(\n",
    "        32, (4, 4), strides=(2, 2), activation='relu'\n",
    "    )(conv_1)\n",
    "    # Flattening the second convolutional layer.\n",
    "    conv_flattened = layers.core.Flatten()(conv_2)\n",
    "    # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "    hidden = layers.Dense(256, activation='relu')(conv_flattened)\n",
    "    # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "    output = layers.Dense(ACTION_SIZE)(hidden)\n",
    "    # Finally, we multiply the output by the mask!\n",
    "    filtered_output = layers.Multiply(name='QValue')([output, actions_input])\n",
    "\n",
    "    model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "    model.summary()\n",
    "    optimizer = RMSprop(lr=FLAGS.learning_rate, rho=0.95, epsilon=0.01)\n",
    "    # model.compile(optimizer, loss='mse')\n",
    "    # to changed model weights more slowly, uses MSE for low values and MAE(Mean Absolute Error) for large values\n",
    "    model.compile(optimizer, loss=huber_loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "# get action from model using epsilon-greedy policy\n",
    "def get_action(history, epsilon, step, model):\n",
    "    if np.random.rand() <= epsilon or step <= FLAGS.observe_step_num:\n",
    "        return random.randrange(ACTION_SIZE)\n",
    "    else:\n",
    "        q_value = model.predict([history, np.ones(ACTION_SIZE).reshape(1, ACTION_SIZE)])\n",
    "        return np.argmax(q_value[0])\n",
    "\n",
    "\n",
    "# save sample <s,a,r,s'> to the replay memory\n",
    "def store_memory(memory, history, action, reward, next_history, dead):\n",
    "    memory.append((history, action, reward, next_history, dead))\n",
    "\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "\n",
    "\n",
    "# train model by radom batch\n",
    "def train_memory_batch(memory, model, log_dir):\n",
    "    mini_batch = random.sample(memory, FLAGS.batch_size)\n",
    "    history = np.zeros((FLAGS.batch_size, ATARI_SHAPE[0],\n",
    "                        ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
    "    next_history = np.zeros((FLAGS.batch_size, ATARI_SHAPE[0],\n",
    "                             ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
    "    target = np.zeros((FLAGS.batch_size,))\n",
    "    action, reward, dead = [], [], []\n",
    "\n",
    "    for idx, val in enumerate(mini_batch):\n",
    "        history[idx] = val[0]\n",
    "        next_history[idx] = val[3]\n",
    "        action.append(val[1])\n",
    "        reward.append(val[2])\n",
    "        dead.append(val[4])\n",
    "\n",
    "    actions_mask = np.ones((FLAGS.batch_size, ACTION_SIZE))\n",
    "    next_Q_values = model.predict([next_history, actions_mask])\n",
    "\n",
    "    # like Q Learning, get maximum Q value at s'\n",
    "    # But from target model\n",
    "    for i in range(FLAGS.batch_size):\n",
    "        if dead[i]:\n",
    "            target[i] = -1\n",
    "            # target[i] = reward[i]\n",
    "        else:\n",
    "            target[i] = reward[i] + FLAGS.gamma * np.amax(next_Q_values[i])\n",
    "\n",
    "    action_one_hot = get_one_hot(action, ACTION_SIZE)\n",
    "    target_one_hot = action_one_hot * target[:, None]\n",
    "\n",
    "    #tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=0,\n",
    "    #                          write_graph=True, write_images=False)\n",
    "\n",
    "    ''''''\n",
    "    h = model.fit(\n",
    "        [history, action_one_hot], target_one_hot, epochs=1,\n",
    "        batch_size=FLAGS.batch_size, verbose=0)\n",
    "        #batch_size=FLAGS.batch_size, verbose=0, callbacks=[tb_callback])\n",
    "\n",
    "    #if h.history['loss'][0] > 10.0:\n",
    "    #    print('too large')\n",
    "\n",
    "    return h.history['loss'][0]\n",
    "\n",
    "\n",
    "def train():\n",
    "    env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "    # deque: Once a bounded length deque is full, when new items are added,\n",
    "    # a corresponding number of items are discarded from the opposite end\n",
    "    memory = deque(maxlen=FLAGS.replay_memory)\n",
    "    episode_number = 0\n",
    "    epsilon = FLAGS.init_epsilon\n",
    "    epsilon_decay = (FLAGS.init_epsilon - FLAGS.final_epsilon) / FLAGS.epsilon_step_num\n",
    "    global_step = 0\n",
    "\n",
    "    if FLAGS.resume:\n",
    "        #model = load_model(FLAGS.restore_file_path)\n",
    "        model = atari_model()\n",
    "        # Assume when we restore the model, the epsilon has already decreased to the final value\n",
    "        epsilon = FLAGS.final_epsilon\n",
    "    else:\n",
    "        model = atari_model()\n",
    "\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    log_dir = \"{}/run-{}-log\".format(FLAGS.train_dir, now)\n",
    "    file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "    model_target = clone_model(model)\n",
    "    model_target.set_weights(model.get_weights())\n",
    "\n",
    "    while episode_number < FLAGS.num_episode:\n",
    "\n",
    "        done = False\n",
    "        dead = False\n",
    "        # 1 episode = 5 lives\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        loss = 0.0\n",
    "        observe = env.reset()\n",
    "\n",
    "        # this is one of DeepMind's idea.\n",
    "        # just do nothing at the start of episode to avoid sub-optimal\n",
    "        for _ in range(random.randint(1, FLAGS.no_op_steps)):\n",
    "            observe, _, _, _ = env.step(1)\n",
    "        # At start of episode, there is no preceding frame\n",
    "        # So just copy initial states to make history\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "        while not done:\n",
    "            if FLAGS.render:\n",
    "                #env.render()\n",
    "                time.sleep(0.01)\n",
    "\n",
    "            # get action for the current history and go one step in environment\n",
    "            action = get_action(history, epsilon, global_step, model_target)\n",
    "            # change action to real_action\n",
    "            real_action = action + 1\n",
    "\n",
    "            # scale down epsilon, the epsilon only begin to decrease after observe steps\n",
    "            if epsilon > FLAGS.final_epsilon and global_step > FLAGS.observe_step_num:\n",
    "                epsilon -= epsilon_decay\n",
    "\n",
    "            observe, reward, done, info = env.step(real_action)\n",
    "            # pre-process the observation --> history\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "            # if the agent missed ball, agent is dead --> episode is not over\n",
    "            if start_life > info['ale.lives']:\n",
    "                dead = True\n",
    "                start_life = info['ale.lives']\n",
    "\n",
    "            # TODO: may be we should give negative reward if miss ball (dead)\n",
    "            # reward = np.clip(reward, -1., 1.)  # clip here is not correct\n",
    "\n",
    "            # save the statue to memory, each replay takes 2 * (84*84*4) bytes = 56448 B = 55.125 KB\n",
    "            store_memory(memory, history, action, reward, next_history, dead)  #\n",
    "\n",
    "            # check if the memory is ready for training\n",
    "            if global_step > FLAGS.observe_step_num:\n",
    "                loss = loss + train_memory_batch(memory, model, log_dir)\n",
    "                # if loss > 100.0:\n",
    "                #    print(loss)\n",
    "                if global_step % FLAGS.refresh_target_model_num == 0:  # update the target model\n",
    "                    model_target.set_weights(model.get_weights())\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            # If agent is dead, set the flag back to false, but keep the history unchanged,\n",
    "            # to avoid to see the ball up in the sky\n",
    "            if dead:\n",
    "                dead = False\n",
    "            else:\n",
    "                history = next_history\n",
    "\n",
    "            #print(\"step: \", global_step)\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                if global_step <= FLAGS.observe_step_num:\n",
    "                    state = \"observe\"\n",
    "                elif FLAGS.observe_step_num < global_step <= FLAGS.observe_step_num + FLAGS.epsilon_step_num:\n",
    "                    state = \"explore\"\n",
    "                else:\n",
    "                    state = \"train\"\n",
    "                print('state: {}, episode: {}, score: {}, global_step: {}, avg loss: {}, step: {}, memory length: {}'\n",
    "                      .format(state, episode_number, score, global_step, loss / float(step), step, len(memory)))\n",
    "\n",
    "                if episode_number % 100 == 0 or (episode_number + 1) == FLAGS.num_episode:\n",
    "                #if episode_number % 1 == 0 or (episode_number + 1) == FLAGS.num_episode:  # debug\n",
    "                    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "                    #file_name = \"breakout_model_{}.h5\".format(now)\n",
    "                    #model_path = os.path.join(FLAGS.train_dir, file_name)\n",
    "                    model.save(FLAGS.restore_file_path)\n",
    "\n",
    "                # Add user custom data to TensorBoard\n",
    "                loss_summary = tf.Summary(\n",
    "                    value=[tf.Summary.Value(tag=\"loss\", simple_value=loss / float(step))])\n",
    "                file_writer.add_summary(loss_summary, global_step=episode_number)\n",
    "\n",
    "                score_summary = tf.Summary(\n",
    "                    value=[tf.Summary.Value(tag=\"score\", simple_value=score)])\n",
    "                file_writer.add_summary(score_summary, global_step=episode_number)\n",
    "\n",
    "                episode_number += 1\n",
    "\n",
    "    file_writer.close()\n",
    "\n",
    "\n",
    "def test():\n",
    "    env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "    episode_number = 0\n",
    "    epsilon = 0.001\n",
    "    global_step = FLAGS.observe_step_num+1\n",
    "    #model = load_model(FLAGS.restore_file_path)\n",
    "    model = load_model(FLAGS.restore_file_path, custom_objects={'huber_loss': huber_loss})  # load model with customized loss func\n",
    "\n",
    "    # test how to deep copy a model\n",
    "    '''\n",
    "    model_copy = clone_model(model)    # only copy the structure, not the value of the weights\n",
    "    model_copy.set_weights(model.get_weights())\n",
    "    '''\n",
    "\n",
    "    while episode_number < FLAGS.num_episode:\n",
    "\n",
    "        done = False\n",
    "        dead = False\n",
    "        # 1 episode = 5 lives\n",
    "        score, start_life = 0, 5\n",
    "        observe = env.reset()\n",
    "\n",
    "        observe, _, _, _ = env.step(1)\n",
    "        # At start of episode, there is no preceding frame\n",
    "        # So just copy initial states to make history\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            time.sleep(0.01)\n",
    "\n",
    "            # get action for the current history and go one step in environment\n",
    "            action = get_action(history, epsilon, global_step, model)\n",
    "            # change action to real_action\n",
    "            real_action = action + 1\n",
    "\n",
    "            observe, reward, done, info = env.step(real_action)\n",
    "            # pre-process the observation --> history\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "            # if the agent missed ball, agent is dead --> episode is not over\n",
    "            if start_life > info['ale.lives']:\n",
    "                dead = True\n",
    "                start_life = info['ale.lives']\n",
    "\n",
    "            # TODO: may be we should give negative reward if miss ball (dead)\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            # If agent is dead, set the flag back to false, but keep the history unchanged,\n",
    "            # to avoid to see the ball up in the sky\n",
    "            if dead:\n",
    "                dead = False\n",
    "            else:\n",
    "                history = next_history\n",
    "\n",
    "            # print(\"step: \", global_step)\n",
    "            global_step += 1\n",
    "\n",
    "            if done:\n",
    "                episode_number += 1\n",
    "                print('episode: {}, score: {}'.format(episode_number, score))\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "     train()\n",
    "    #test()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paperspace/fastai/courses/tim\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
